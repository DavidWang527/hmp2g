{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[client]: sending np.array([1,2,3])\n",
      "[client]: sending np.array([4,5,6])\n",
      "[server]: recv [array([1, 2, 3]), array([4, 5, 6])]\n"
     ]
    }
   ],
   "source": [
    "from UTILS.network import TcpServerP2P, TcpClientP2P\n",
    "import threading, time\n",
    "import numpy as np\n",
    "ipport = ('127.0.0.1', 25455)\n",
    "server = TcpServerP2P(ipport, obj='pickle')\n",
    "client = TcpClientP2P(ipport, obj='pickle')\n",
    "def server_fn():\n",
    "    time.sleep(1)\n",
    "    data = server.wait_next_dgram()\n",
    "    print('[server]: recv', data)\n",
    "    server.reply_last_client(np.array([4,5,6]))\n",
    "    print('[server]: done reply')\n",
    "\n",
    "def noreply_server_fn():\n",
    "    time.sleep(1)\n",
    "    data = server.wait_multi_dgrams()\n",
    "    print('[server]: recv', data)\n",
    "\n",
    "\n",
    "def client_fn():\n",
    "    print('[client]: sending np.array([1,2,3])')\n",
    "    rep = client.send_and_wait_reply(np.array([1,2,3]))\n",
    "    print('[client]: recv', rep)\n",
    "\n",
    "def nowait_client_fn():\n",
    "    print('[client]: sending np.array([1,2,3])')\n",
    "    rep = client.send_dgram_to_target(np.array([1,2,3]))\n",
    "    print('[client]: sending np.array([4,5,6])')\n",
    "    rep = client.send_dgram_to_target(np.array([4,5,6]))\n",
    "    \n",
    "thread_hi = threading.Thread(target=noreply_server_fn)\n",
    "thread_hello = threading.Thread(target=nowait_client_fn)\n",
    "# 启动线程\n",
    "thread_hi.start()\n",
    "thread_hello.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h两个动作的概率: tensor([0.3941, 0.6059])\n",
      "h动作为(1为采样, 0为贪婪最大): 1\n",
      "选定h动作的概率: tensor([0.6059])\n",
      "act动作概率: tensor([0.4640, 0.2814, 0.2546])\n",
      "act动作选择 tensor(2)\n",
      "选定act动作选择的概率: tensor([0.2546])\n",
      "actProbs: tensor([0.1543])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    h两个动作的概率: tensor([0.3941, 0.6059])\\n    h动作为(1为采样, 0为贪婪最大): 0\\n    选定h动作的概率: tensor([0.3941])\\n    act动作概率: tensor([0.4640, 0.2814, 0.2546])\\n    act动作选择 tensor(0)\\n    选定act动作选择的概率: tensor([0.4640])\\n    actProbs: tensor([0.6752])\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def _get_act_log_probs(distribution, action):\n",
    "    return distribution.log_prob(action.squeeze(-1)).unsqueeze(-1)\n",
    "\n",
    " \n",
    "hyper_act_logits = torch.Tensor([0.32, 0.75])\n",
    "logits_agent_cluster = torch.Tensor([0.7, 0.2, 0.1])\n",
    "test_mode = False\n",
    "eval_mode = False\n",
    "\n",
    "h_dist = Categorical(logits = hyper_act_logits)\n",
    "print('h两个动作的概率:', h_dist.probs)\n",
    "if not test_mode:  h_act = h_dist.sample() if not eval_mode else eval_actions\n",
    "else:              h_act = torch.argmax(h_dist.probs)\n",
    "print('h动作为(1为采样, 0为贪婪最大):', h_act.item())\n",
    "# SampleLogProb = torch.log(h_dist.probs[..., 1])\n",
    "SampleLogProb = _get_act_log_probs(h_dist, torch.ones_like(h_act))\n",
    "hActLogProbsRef = _get_act_log_probs(h_dist, h_act)\n",
    "print('选定h动作的概率:', torch.exp(hActLogProbsRef))\n",
    "\n",
    "\n",
    "act_dist = Categorical(logits = logits_agent_cluster)\n",
    "print('act动作概率:', act_dist.probs)\n",
    "act_sample = act_dist.sample() if not eval_mode else eval_actions\n",
    "act_argmax = torch.argmax(act_dist.probs)\n",
    "\n",
    "act = torch.where(h_act==1, act_sample, act_argmax) # h_act: shape=($n_thread, $n_agent)\n",
    "\n",
    "################# 至此，已经选出了最后的动作 ################\n",
    "\n",
    "sel_argmax = (act_argmax==act)\n",
    "print('act动作选择', act)\n",
    "\n",
    "actLogProbs01 = _get_act_log_probs(act_dist, act) # the policy gradient loss will feedback from here\n",
    "print('选定act动作选择的概率:', torch.exp(actLogProbs01))\n",
    "\n",
    "\n",
    "actLogProbs_notargmax = actLogProbs01 + SampleLogProb\n",
    "actLogProbs_argmax = torch.log(torch.exp(actLogProbs01 + SampleLogProb) + (1-torch.exp(SampleLogProb)))\n",
    "actLogProbs = torch.where(sel_argmax, actLogProbs_argmax, actLogProbs_notargmax)\n",
    "print('actProbs:', torch.exp(actLogProbs))\n",
    "\n",
    "'''\n",
    "    h两个动作的概率: tensor([0.3941, 0.6059])\n",
    "    h动作为(1为采样, 0为贪婪最大): 0\n",
    "    选定h动作的概率: tensor([0.3941])\n",
    "    act动作概率: tensor([0.4640, 0.2814, 0.2546])\n",
    "    act动作选择 tensor(0)\n",
    "    选定act动作选择的概率: tensor([0.4640])\n",
    "    actProbs: tensor([0.6752])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296\n",
      "0.1399\n",
      "0.1305\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def _get_act_log_probs(distribution, action):\n",
    "    return distribution.log_prob(action.squeeze(-1)).unsqueeze(-1)\n",
    "\n",
    "res = []\n",
    "for i in range(10000):\n",
    "\n",
    "    hyper_act_logits = torch.Tensor([0.5, 0.5])\n",
    "    logits_agent_cluster = torch.Tensor([0.7, 0.2, 0.1])\n",
    "    test_mode = False\n",
    "    eval_mode = False\n",
    "\n",
    "    h_dist = Categorical(logits = hyper_act_logits)\n",
    "    # print('h_dist:', h_dist.probs)\n",
    "    if not test_mode:  h_act = h_dist.sample() if not eval_mode else eval_actions\n",
    "    else:              h_act = torch.argmax(h_dist.probs)\n",
    "    # print('h_act(1为采样, 0为最大):', h_act)\n",
    "\n",
    "    hActLogProbs = _get_act_log_probs(h_dist, h_act)\n",
    "    # print('hActProbs:', torch.exp(hActLogProbs))\n",
    "\n",
    "\n",
    "    act_dist = Categorical(logits = logits_agent_cluster)\n",
    "    # print('act_dist:', act_dist.probs)\n",
    "    act_sample = act_dist.sample() if not eval_mode else eval_actions\n",
    "    act_argmax = torch.argmax(act_dist.probs)\n",
    "\n",
    "    act = torch.where(h_act==1, act_sample, act_argmax) # h_act: shape=($n_thread, $n_agent)\n",
    "    sel_argmax = (act_argmax==act)\n",
    "    # print('act:', act)\n",
    "\n",
    "    actLogProbs01 = _get_act_log_probs(act_dist, act) # the policy gradient loss will feedback from here\n",
    "    # print('actProbs01:', torch.exp(actLogProbs01))\n",
    "\n",
    "\n",
    "    actLogProbs_notargmax = actLogProbs01 + hActLogProbs\n",
    "    actLogProbs_argmax = torch.log(torch.exp(actLogProbs01 + hActLogProbs) + (1-torch.exp(hActLogProbs)))\n",
    "    actLogProbs = torch.where(sel_argmax, actLogProbs_argmax, actLogProbs_notargmax)\n",
    "    # print('actProbs:', torch.exp(actLogProbs))\n",
    "    res.append(act.item())\n",
    "res = np.array(res)\n",
    "len_res = len(res)\n",
    "print(sum(res==0)/len_res)\n",
    "print(sum(res==1)/len_res)\n",
    "print(sum(res==2)/len_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.send_targeted_dgram('ddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def _get_act_log_probs(distribution, action):\n",
    "    return distribution.log_prob(action.squeeze(-1)).unsqueeze(-1)\n",
    "\n",
    " \n",
    "hyper_act_logits = torch.Tensor([0.5, 0.5])\n",
    "logits_agent_cluster = torch.Tensor([0.7, 0.2, 0.1])\n",
    "test_mode = False\n",
    "eval_mode = False\n",
    "\n",
    "h_dist = Categorical(logits = hyper_act_logits)\n",
    "if not test_mode:  h_act = h_dist.sample() if not eval_mode else eval_actions\n",
    "else:              h_act = torch.argmax(h_dist.probs)\n",
    "hActLogProbs = _get_act_log_probs(h_dist, h_act)\n",
    "\n",
    "act_dist = Categorical(logits = logits_agent_cluster)\n",
    "act_sample = act_dist.sample() if not eval_mode else eval_actions\n",
    "act_argmax = torch.argmax(act_dist.probs)\n",
    "\n",
    "act = torch.where(h_act==1, act_sample, act_argmax) # h_act: shape=($n_thread, $n_agent)\n",
    "sel_argmax = (act_argmax==act)\n",
    "\n",
    "actLogProbs01 = _get_act_log_probs(act_dist, act) # the policy gradient loss will feedback from here\n",
    "\n",
    "actLogProbs_notargmax = actLogProbs01 + hActLogProbs\n",
    "actLogProbs_argmax = torch.log(torch.exp(actLogProbs01 + hActLogProbs) + (1-torch.exp(hActLogProbs)))\n",
    "actLogProbs = torch.where(sel_argmax, actLogProbs_argmax, actLogProbs_notargmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from UTILS.tensor_ops import _2tensor, cfg\n",
    "cfg.device_ = 'cuda:0'\n",
    "cfg.use_float64_ = False\n",
    "cfg.init = True\n",
    "\n",
    "input_dim = 10\n",
    "my_data_sample_x = np.random.rand(1000,10)*10 - 1 # random from -1 to +1\n",
    "my_data_sample_y = my_data_sample_x.mean(-1)**2 + my_data_sample_x.mean(-1)*(-2) + 1\n",
    "\n",
    "# solve the weight inital problem\n",
    "vae_mod = VanillaVAE(input_dim=input_dim, latent_dim=16, hidden_dim=32, degenerate2ae=False)\n",
    "my_data_sample_x = _2tensor(my_data_sample_x)\n",
    "vae_mod = _2tensor(vae_mod)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(vae_mod.parameters(), lr=3e-3)\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae_mod.train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    overall_loss = 0\n",
    "    x = my_data_sample_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "    x_hat, x_origin, mean, log_var = vae_mod(x) # model(x)\n",
    "    error = ( torch.abs(x_hat)-torch.abs(x) )/( torch.abs(x)+1e-9 )\n",
    "    std_, mean_ = torch.std_mean(error, unbiased=False)\n",
    "    print('\\test error mean %.2f and std %.2f'%(mean_.item(), std_.item()) )\n",
    "\n",
    "\n",
    "    lossdict = vae_mod.loss_function(x=x, x_hat=x_hat, mean=mean, log_var=log_var, kld_loss_weight=1, recons_loss_weight=1)\n",
    "    loss = lossdict['loss']\n",
    "    overall_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 'Reconstruction_Loss':recons_loss.detach(), 'KLD'\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tReconstruction_Loss: \", lossdict['Reconstruction_Loss'].item(), \"\\tKLD: \", lossdict['KLD'].item())\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "# from abc import abstractmethod\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 degenerate2ae = False,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        self.training = True\n",
    "\n",
    "        # Part 1, encoder\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Part 2, decoder\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.degenerate2ae = degenerate2ae\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        h_       = self.LeakyReLU(self.FC_input(input))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_) \n",
    "\n",
    "        return mean, log_var\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        h     = self.LeakyReLU(self.FC_hidden(z))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = self.FC_output(h)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        if not self.degenerate2ae:\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "        else:\n",
    "            z = mu\n",
    "        return  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "\n",
    "    def loss_function(self,\n",
    "                      x, x_hat, mean, log_var, kld_loss_weight, recons_loss_weight,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = x_hat\n",
    "        input = x\n",
    "        mu = mean\n",
    "        log_var = log_var\n",
    "\n",
    "        # kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        # 计算高斯分布和标准正态分布的KL散度\n",
    "        if not self.degenerate2ae:\n",
    "            kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "            loss = recons_loss*recons_loss_weight + kld_loss*kld_loss_weight\n",
    "        else:\n",
    "            kld_loss = torch.zeros_like(recons_loss)\n",
    "            loss = recons_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "\n",
    "    # Do I need this?\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    # Do I need this?\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "import numpy as np\n",
    "from UTILS.tensor_ops import _2tensor, cfg\n",
    "cfg.device_ = 'cuda:0'\n",
    "cfg.use_float64_ = False\n",
    "cfg.init = True\n",
    "\n",
    "input_dim = 10\n",
    "my_data_sample_x = (np.random.rand(1000,10)-0.5)*2 + 5 # 0.75~1.25\n",
    "my_data_sample_y = my_data_sample_x.mean(-1)**2 + my_data_sample_x.mean(-1)*(-2) + 1\n",
    "\n",
    "# solve the weight inital problem\n",
    "vae_mod = VanillaVAE(input_dim=input_dim, latent_dim=16, hidden_dim=32, degenerate2ae=False)\n",
    "my_data_sample_x = _2tensor(my_data_sample_x)\n",
    "vae_mod = _2tensor(vae_mod)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(vae_mod.parameters(), lr=3e-3)\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae_mod.train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    overall_loss = 0\n",
    "    x = my_data_sample_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "    x_hat, x_origin, mean, log_var = vae_mod(x) # model(x)\n",
    "    error = ( torch.abs(x_hat - x) )/( torch.abs(x)+1e-9 )\n",
    "    std_, mean_ = torch.std_mean(error, unbiased=False)\n",
    "    print('\\test error mean %.2f%% and std %.2f'%(mean_.item()*100, std_.item()) , end='\\t')\n",
    "\n",
    "\n",
    "    lossdict = vae_mod.loss_function(x=x, x_hat=x_hat, mean=mean, log_var=log_var, kld_loss_weight=1, recons_loss_weight=1)\n",
    "    loss = lossdict['loss']\n",
    "    overall_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 'Reconstruction_Loss':recons_loss.detach(), 'KLD'\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tReconstruction_Loss: \", lossdict['Reconstruction_Loss'].item(), \"\\tKLD: \", lossdict['KLD'].item())\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
