{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.most_recent_client RECYCLE/Sockets/unix/f0f17a80ad0911ec9e623cecef04a859_client_f0f1e\n",
      "recv from : RECYCLE/Sockets/unix/f0f17a80ad0911ec9e623cecef04a859_client_f0f1e  data : [1 2 3]\n",
      "reply_last_client : RECYCLE/Sockets/unix/f0f17a80ad0911ec9e623cecef04a859_client_f0f1e  data : [4 5 6]\n",
      "get_reply : RECYCLE/Sockets/unix/f0f17a80ad0911ec9e623cecef04a859  data : [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import socket, threading, pickle, uuid, os\n",
    "import numpy as np\n",
    "BUFSIZE = 1024\n",
    "DEBUG_NETWORK = True\n",
    "class UnixUdpServer:\n",
    "    def __init__(self, unix_path, obj='bytes') -> None:\n",
    "        try: os.makedirs(os.path.dirname(unix_path))\n",
    "        except: pass\n",
    "        self.unix_path = unix_path\n",
    "        self.server = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n",
    "        self.server.bind(self.unix_path)\n",
    "        self.most_recent_client = None\n",
    "        self.use_pickle = (obj=='pickle')\n",
    "        self.convert_str = (obj=='str')\n",
    "        return\n",
    "\n",
    "    def wait_next_dgram(self):\n",
    "        data, self.most_recent_client = self.server.recvfrom(BUFSIZE)\n",
    "        print('self.most_recent_client',self.most_recent_client)\n",
    "        if self.convert_str: data = data.decode('utf8')\n",
    "        if self.use_pickle: data = pickle.loads(data)\n",
    "        if DEBUG_NETWORK: print('recv from :', self.most_recent_client, ' data :', data)\n",
    "        return data\n",
    "\n",
    "    def reply_last_client(self, data):\n",
    "        assert self.most_recent_client is not None\n",
    "        if DEBUG_NETWORK: print('reply_last_client :', self.most_recent_client, ' data :', data)\n",
    "        if self.use_pickle: data = pickle.dumps(data)\n",
    "        if self.convert_str: data = bytes(data, encoding='utf8')\n",
    "        self.server.sendto(data, self.most_recent_client)\n",
    "        return\n",
    "\n",
    "    def __del__(self):\n",
    "        self.server.close()\n",
    "        return\n",
    "\n",
    "class UnixUdpTargetedClient:\n",
    "    def __init__(self, target_unix_path, self_unix_path=None, obj='bytes') -> None:\n",
    "        self.target_unix_path = target_unix_path\n",
    "        if self_unix_path is not None:\n",
    "            self.self_unix_path = self_unix_path  \n",
    "        else:\n",
    "            self.self_unix_path = target_unix_path+'_client_'+uuid.uuid1().hex[:5]\n",
    "        self.client = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)\n",
    "        self.client.bind(self.self_unix_path)\n",
    "        self.use_pickle = (obj=='pickle')\n",
    "        self.convert_str = (obj=='str')\n",
    "        return\n",
    "\n",
    "    def send_dgram_to_target(self, data):\n",
    "        if self.use_pickle: data = pickle.dumps(data)\n",
    "        if self.convert_str: data = bytes(data, encoding='utf8')\n",
    "        self.client.sendto(data, self.target_unix_path)\n",
    "        if DEBUG_NETWORK: print('send_targeted_dgram :', self.target_unix_path, ' data :', data)\n",
    "        return\n",
    "\n",
    "    def send_and_wait_reply(self, data):\n",
    "        if self.use_pickle: data = pickle.dumps(data)\n",
    "        if self.convert_str: data = bytes(data, encoding='utf8')\n",
    "        self.client.sendto(data, self.target_unix_path)\n",
    "        data, _ = self.client.recvfrom(BUFSIZE)\n",
    "        if self.convert_str: data = data.decode('utf8')\n",
    "        if self.use_pickle: data = pickle.loads(data)\n",
    "        if DEBUG_NETWORK: print('get_reply :', self.target_unix_path, ' data :', data)\n",
    "        return data\n",
    "\n",
    "remote_uuid = uuid.uuid1().hex   # use uuid to identify threads\n",
    "\n",
    "unix_path = 'RECYCLE/Sockets/unix/%s'%remote_uuid\n",
    "server = UnixUdpServer(unix_path, obj='pickle')\n",
    "client = UnixUdpTargetedClient(unix_path, obj='pickle')\n",
    "\n",
    "def server_fn():\n",
    "    data = server.wait_next_dgram()\n",
    "    server.reply_last_client(np.array([4,5,6]))\n",
    "\n",
    "def client_fn():\n",
    "    rep = client.send_and_wait_reply(np.array([1,2,3]))\n",
    "\n",
    "\n",
    "thread_hi = threading.Thread(target=server_fn)\n",
    "thread_hello = threading.Thread(target=client_fn)\n",
    "# 启动线程\n",
    "thread_hi.start()\n",
    "thread_hello.start()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RECYCLE/Sockets/9a64f35ead0911ec9e623cecef04a859'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unix_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.send_targeted_dgram('ddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "# from abc import abstractmethod\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 degenerate2ae = False,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        self.training = True\n",
    "\n",
    "        # Part 1, encoder\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Part 2, decoder\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.degenerate2ae = degenerate2ae\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        h_       = self.LeakyReLU(self.FC_input(input))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_) \n",
    "\n",
    "        return mean, log_var\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        h     = self.LeakyReLU(self.FC_hidden(z))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = self.FC_output(h)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        if not self.degenerate2ae:\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "        else:\n",
    "            z = mu\n",
    "        return  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "\n",
    "    def loss_function(self,\n",
    "                      x, x_hat, mean, log_var, kld_loss_weight, recons_loss_weight,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = x_hat\n",
    "        input = x\n",
    "        mu = mean\n",
    "        log_var = log_var\n",
    "\n",
    "        # kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "        # 计算高斯分布和标准正态分布的KL散度\n",
    "        if not self.degenerate2ae:\n",
    "            kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "            loss = recons_loss*recons_loss_weight + kld_loss*kld_loss_weight\n",
    "        else:\n",
    "            kld_loss = torch.zeros_like(recons_loss)\n",
    "            loss = recons_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "\n",
    "    # Do I need this?\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    # Do I need this?\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from UTILS.tensor_ops import _2tensor, cfg\n",
    "cfg.device_ = 'cuda:0'\n",
    "cfg.use_float64_ = False\n",
    "cfg.init = True\n",
    "\n",
    "input_dim = 10\n",
    "my_data_sample_x = np.random.rand(1000,10)*10 - 1 # random from -1 to +1\n",
    "my_data_sample_y = my_data_sample_x.mean(-1)**2 + my_data_sample_x.mean(-1)*(-2) + 1\n",
    "\n",
    "# solve the weight inital problem\n",
    "vae_mod = VanillaVAE(input_dim=input_dim, latent_dim=16, hidden_dim=32, degenerate2ae=False)\n",
    "my_data_sample_x = _2tensor(my_data_sample_x)\n",
    "vae_mod = _2tensor(vae_mod)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(vae_mod.parameters(), lr=3e-3)\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae_mod.train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    overall_loss = 0\n",
    "    x = my_data_sample_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "    x_hat, x_origin, mean, log_var = vae_mod(x) # model(x)\n",
    "    error = ( torch.abs(x_hat)-torch.abs(x) )/( torch.abs(x)+1e-9 )\n",
    "    std_, mean_ = torch.std_mean(error, unbiased=False)\n",
    "    print('\\test error mean %.2f and std %.2f'%(mean_.item(), std_.item()) )\n",
    "\n",
    "\n",
    "    lossdict = vae_mod.loss_function(x=x, x_hat=x_hat, mean=mean, log_var=log_var, kld_loss_weight=1, recons_loss_weight=1)\n",
    "    loss = lossdict['loss']\n",
    "    overall_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 'Reconstruction_Loss':recons_loss.detach(), 'KLD'\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tReconstruction_Loss: \", lossdict['Reconstruction_Loss'].item(), \"\\tKLD: \", lossdict['KLD'].item())\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "# from abc import abstractmethod\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 degenerate2ae = False,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        self.training = True\n",
    "\n",
    "        # Part 1, encoder\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Part 2, decoder\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.degenerate2ae = degenerate2ae\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        h_       = self.LeakyReLU(self.FC_input(input))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_) \n",
    "\n",
    "        return mean, log_var\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        h     = self.LeakyReLU(self.FC_hidden(z))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = self.FC_output(h)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        if not self.degenerate2ae:\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "        else:\n",
    "            z = mu\n",
    "        return  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "\n",
    "    def loss_function(self,\n",
    "                      x, x_hat, mean, log_var, kld_loss_weight, recons_loss_weight,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = x_hat\n",
    "        input = x\n",
    "        mu = mean\n",
    "        log_var = log_var\n",
    "\n",
    "        # kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        # 计算高斯分布和标准正态分布的KL散度\n",
    "        if not self.degenerate2ae:\n",
    "            kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "            loss = recons_loss*recons_loss_weight + kld_loss*kld_loss_weight\n",
    "        else:\n",
    "            kld_loss = torch.zeros_like(recons_loss)\n",
    "            loss = recons_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "\n",
    "    # Do I need this?\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    # Do I need this?\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "import numpy as np\n",
    "from UTILS.tensor_ops import _2tensor, cfg\n",
    "cfg.device_ = 'cuda:0'\n",
    "cfg.use_float64_ = False\n",
    "cfg.init = True\n",
    "\n",
    "input_dim = 10\n",
    "my_data_sample_x = (np.random.rand(1000,10)-0.5)*2 + 5 # 0.75~1.25\n",
    "my_data_sample_y = my_data_sample_x.mean(-1)**2 + my_data_sample_x.mean(-1)*(-2) + 1\n",
    "\n",
    "# solve the weight inital problem\n",
    "vae_mod = VanillaVAE(input_dim=input_dim, latent_dim=16, hidden_dim=32, degenerate2ae=False)\n",
    "my_data_sample_x = _2tensor(my_data_sample_x)\n",
    "vae_mod = _2tensor(vae_mod)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(vae_mod.parameters(), lr=3e-3)\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae_mod.train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    overall_loss = 0\n",
    "    x = my_data_sample_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "    x_hat, x_origin, mean, log_var = vae_mod(x) # model(x)\n",
    "    error = ( torch.abs(x_hat - x) )/( torch.abs(x)+1e-9 )\n",
    "    std_, mean_ = torch.std_mean(error, unbiased=False)\n",
    "    print('\\test error mean %.2f%% and std %.2f'%(mean_.item()*100, std_.item()) , end='\\t')\n",
    "\n",
    "\n",
    "    lossdict = vae_mod.loss_function(x=x, x_hat=x_hat, mean=mean, log_var=log_var, kld_loss_weight=1, recons_loss_weight=1)\n",
    "    loss = lossdict['loss']\n",
    "    overall_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 'Reconstruction_Loss':recons_loss.detach(), 'KLD'\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tReconstruction_Loss: \", lossdict['Reconstruction_Loss'].item(), \"\\tKLD: \", lossdict['KLD'].item())\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
