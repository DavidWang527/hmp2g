{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 1 1 0 1 1 0 1 1]\n",
      "[3 2 0 0 0 0 2 1 1 2 3 1 2 3 1]\n",
      "[7 2 0 0 4 4 2 1 5 6 3 5 6 3 1]\n",
      "[ 7 10  8  0  4 12  2  1  5 14 11 13  6  3  9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_division_tree(n_agents):\n",
    "    agent2divitreeindex = np.arange(n_agents)\n",
    "    np.random.shuffle(agent2divitreeindex)\n",
    "    max_div = np.ceil(np.log2(n_agents)).astype(int)\n",
    "    levels = np.zeros(shape=(max_div+1, n_agents), dtype=int)\n",
    "    tree_of_agent = []*(max_div+1)\n",
    "    for ith, level in enumerate(levels):\n",
    "        if ith == 0: continue\n",
    "        levels[ith,:]  = levels[ith-1,:]\n",
    "        for j in range(n_agents):\n",
    "            seg = j // ( n_agents /2**ith)\n",
    "            if seg%2==1:\n",
    "                levels[ith,j] += 2**(ith-1)\n",
    "    res_levels = levels.copy()\n",
    "    for i, div_tree_index in enumerate(agent2divitreeindex):\n",
    "        res_levels[:, i] = levels[:, div_tree_index]\n",
    "    return res_levels\n",
    "\n",
    "\n",
    "res = get_division_tree(15)\n",
    "for r in res:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1683108/3426538403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent2divitreeindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_agents' is not defined"
     ]
    }
   ],
   "source": [
    "agent2divitreeindex = np.arange(n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296\n",
      "0.1399\n",
      "0.1305\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def _get_act_log_probs(distribution, action):\n",
    "    return distribution.log_prob(action.squeeze(-1)).unsqueeze(-1)\n",
    "\n",
    "res = []\n",
    "for i in range(10000):\n",
    "\n",
    "    hyper_act_logits = torch.Tensor([0.5, 0.5])\n",
    "    logits_agent_cluster = torch.Tensor([0.7, 0.2, 0.1])\n",
    "    test_mode = False\n",
    "    eval_mode = False\n",
    "\n",
    "    h_dist = Categorical(logits = hyper_act_logits)\n",
    "    # print('h_dist:', h_dist.probs)\n",
    "    if not test_mode:  h_act = h_dist.sample() if not eval_mode else eval_actions\n",
    "    else:              h_act = torch.argmax(h_dist.probs)\n",
    "    # print('h_act(1为采样, 0为最大):', h_act)\n",
    "\n",
    "    hActLogProbs = _get_act_log_probs(h_dist, h_act)\n",
    "    # print('hActProbs:', torch.exp(hActLogProbs))\n",
    "\n",
    "\n",
    "    act_dist = Categorical(logits = logits_agent_cluster)\n",
    "    # print('act_dist:', act_dist.probs)\n",
    "    act_sample = act_dist.sample() if not eval_mode else eval_actions\n",
    "    act_argmax = torch.argmax(act_dist.probs)\n",
    "\n",
    "    act = torch.where(h_act==1, act_sample, act_argmax) # h_act: shape=($n_thread, $n_agent)\n",
    "    sel_argmax = (act_argmax==act)\n",
    "    # print('act:', act)\n",
    "\n",
    "    actLogProbs01 = _get_act_log_probs(act_dist, act) # the policy gradient loss will feedback from here\n",
    "    # print('actProbs01:', torch.exp(actLogProbs01))\n",
    "\n",
    "\n",
    "    actLogProbs_notargmax = actLogProbs01 + hActLogProbs\n",
    "    actLogProbs_argmax = torch.log(torch.exp(actLogProbs01 + hActLogProbs) + (1-torch.exp(hActLogProbs)))\n",
    "    actLogProbs = torch.where(sel_argmax, actLogProbs_argmax, actLogProbs_notargmax)\n",
    "    # print('actProbs:', torch.exp(actLogProbs))\n",
    "    res.append(act.item())\n",
    "res = np.array(res)\n",
    "len_res = len(res)\n",
    "print(sum(res==0)/len_res)\n",
    "print(sum(res==1)/len_res)\n",
    "print(sum(res==2)/len_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.send_targeted_dgram('ddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_agent_cluster = torch.Tensor([0.7, 0.2, 0.1])\n",
    "# torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=- 1)\n",
    "q = {0:0,1:0,2:0}\n",
    "for i in range(10000):\n",
    "    torch.set_printoptions(precision=3, sci_mode=False)\n",
    "    t = torch.nn.functional.gumbel_softmax(logits_agent_cluster, tau=0.2).argmax().item()\n",
    "    q[t] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4670, 1: 2757, 2: 2573}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.464, 0.281, 0.255])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.Tensor([0.7, 0.2, 0.1]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    U = U.cuda()\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature=1):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1, hard=False):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if not hard:\n",
    "        return y\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
