{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self.run()    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "\n",
      "  File \"<ipython-input-4-df7357bea1a0>\", line 147, in client_fn\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-df7357bea1a0>\", line 143, in server_fn\n",
      "  File \"<ipython-input-4-df7357bea1a0>\", line 129, in send_and_wait_reply\n",
      "  File \"<ipython-input-4-df7357bea1a0>\", line 82, in wait_next_dgram\n",
      "  File \"<ipython-input-4-df7357bea1a0>\", line 15, in lower_send\n",
      "  File \"<ipython-input-4-df7357bea1a0>\", line 20, in lower_recv\n",
      "AttributeError: 'NoneType' object has no attribute 'send'\n",
      "AttributeError: 'tuple' object has no attribute 'recv'\n"
     ]
    }
   ],
   "source": [
    "import socket, threading, pickle, uuid, os, atexit\n",
    "import socketserver\n",
    "import numpy as np\n",
    "BUFSIZE = 10485760\n",
    "TCPBUFSIZE = 10485760\n",
    "DEBUG_NETWORK = True\n",
    "\n",
    "class StreamingPackageSep:\n",
    "    def __init__(self):\n",
    "        self.buff = [b'']\n",
    "        self.myEOF = b'\\xaa\\x55\\xaaHMP\\xaa\\x55'    # those bytes follow 010101 or 101010 pattern\n",
    "        # self.myEOF = b'#A5@5A#'    # the EOF string for frame seperation\n",
    "\n",
    "    def lower_send(self, data, connection):\n",
    "        if DEBUG_NETWORK: assert self.myEOF not in data, 'This is (almost) not possible!'\n",
    "        data = data + self.myEOF\n",
    "        if DEBUG_NETWORK: print('data length:', len(data))\n",
    "        connection.send(data)\n",
    "\n",
    "    # Fox-Protocal\n",
    "    def lower_recv(self, connection):\n",
    "        while True:\n",
    "            recvData = connection.recv(BUFSIZE)\n",
    "            # ends_with_mark = recvData.endswith(self.myEOF)\n",
    "            split_res = recvData.split(self.myEOF)\n",
    "            assert len(split_res) != 0\n",
    "            if len(split_res) == 1:\n",
    "                # 说明没有终止符，直接将结果贴到buf最后一项\n",
    "                self.buff[-1] = self.buff[-1] + split_res[0]\n",
    "                if self.myEOF in self.buff[-1]: self.handle_flag_breakdown()\n",
    "            else:\n",
    "                n_split = len(split_res)\n",
    "                for i, r in enumerate(split_res):\n",
    "                    self.buff[-1] = self.buff[-1] + r   # 追加buff\n",
    "                    if i == 0 and (self.myEOF in self.buff[-1]): \n",
    "                        # 第一次追加后，在修复的数据断面上发现了myEOF！\n",
    "                        self.handle_flag_breakdown()\n",
    "                    if i != n_split-1: \n",
    "                        # starts a new entry\n",
    "                        self.buff.append(b'')\n",
    "                    else:  \n",
    "                        # i == n_split-1, which is the last item\n",
    "                        if r == b'': continue\n",
    "            if len(self.buff)>=2:\n",
    "                # 数据成型，拿取成型的数据\n",
    "                buff_list = self.buff[:-1]  \n",
    "                self.buff = self.buff[-1:]\n",
    "                break\n",
    "\n",
    "        assert len(buff_list) == 1, ('一次拿到了两帧数据, 不符合预想')\n",
    "        data = buff_list[0] # str(, encoding = \"utf-8\")\n",
    "        return data, connection\n",
    "\n",
    "    def handle_flag_breakdown(self):\n",
    "        split_ = self.buff[-1].split(self.myEOF)\n",
    "        assert len(split_)==2\n",
    "        self.buff[-1] = split_[0]\n",
    "        # starts a new entry\n",
    "        self.buff.append(b'')\n",
    "        self.buff[-1] = split_[1]\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "# send() is used for TCP SOCK_STREAM connected sockets, and sendto() is used for UDP SOCK_DGRAM unconnected datagram sockets\n",
    "class UnixTcpServerP2P(StreamingPackageSep):\n",
    "    def __init__(self, unix_path, obj='bytes') -> None:\n",
    "        super().__init__()\n",
    "        try: os.makedirs(os.path.dirname(unix_path))\n",
    "        except: pass\n",
    "        self.unix_path = unix_path\n",
    "        self.server = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n",
    "        self.server.bind(self.unix_path)\n",
    "        self.server.listen()\n",
    "        self.most_recent_client = None\n",
    "        self.use_pickle = (obj=='pickle')\n",
    "        self.convert_str = (obj=='str')\n",
    "        atexit.register(self.__del__)\n",
    "\n",
    "    def accept_conn(self):\n",
    "        conn, _  = self.server.accept()\n",
    "        return conn\n",
    "\n",
    "    def wait_next_dgram(self):\n",
    "        if self.most_recent_client is None: self.most_recent_client, _ = self.server.accept()\n",
    "        data, self.most_recent_client = self.lower_recv(self.most_recent_client)\n",
    "        print('self.most_recent_client',self.most_recent_client)\n",
    "        if self.convert_str: data = data.decode('utf8')\n",
    "        if self.use_pickle: data = pickle.loads(data)\n",
    "        if DEBUG_NETWORK: print('recv from :', self.most_recent_client, ' data :', data)\n",
    "        return data\n",
    "\n",
    "    def reply_last_client(self, data):\n",
    "        assert self.most_recent_client is not None\n",
    "        if DEBUG_NETWORK: print('reply_last_client :', self.most_recent_client, ' data :', data)\n",
    "        if self.use_pickle: data = pickle.dumps(data)\n",
    "        if self.convert_str: data = bytes(data, encoding='utf8')\n",
    "        self.lower_send(data, self.most_recent_client)\n",
    "        return\n",
    "\n",
    "    def __del__(self):\n",
    "        self.server.close()\n",
    "        os.remove(self.unix_path)\n",
    "        return\n",
    "\n",
    "\n",
    "class UnixTcpClientP2P(StreamingPackageSep):\n",
    "    def __init__(self, target_unix_path, self_unix_path=None, obj='bytes') -> None:\n",
    "        super().__init__()\n",
    "        self.target_unix_path = target_unix_path\n",
    "        if self_unix_path is not None:\n",
    "            self.self_unix_path = self_unix_path  \n",
    "        else:\n",
    "            self.self_unix_path = target_unix_path+'_client_'+uuid.uuid1().hex[:5]\n",
    "        self.client = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n",
    "        self.client.bind(self.self_unix_path)\n",
    "        self.use_pickle = (obj=='pickle')\n",
    "        self.convert_str = (obj=='str')\n",
    "        self.connected = False\n",
    "        atexit.register(self.__del__)\n",
    "\n",
    "    def send_dgram_to_target(self, data):\n",
    "        if self.use_pickle: data = pickle.dumps(data)\n",
    "        if self.convert_str: data = bytes(data, encoding='utf8')\n",
    "        if not self.connected: self.client.connect(self.target_unix_path); self.connected = True\n",
    "        self.lower_send(data, self.client)\n",
    "        if DEBUG_NETWORK: print('send_targeted_dgram :', self.client, ' data :', data)\n",
    "        return\n",
    "\n",
    "    def send_and_wait_reply(self, data):\n",
    "        if self.use_pickle: data = pickle.dumps(data)\n",
    "        if self.convert_str: data = bytes(data, encoding='utf8')\n",
    "        if not self.connected: self.client.connect(self.target_unix_path); self.connected = True\n",
    "        self.lower_send(data, self.client)\n",
    "        data, _ = self.lower_recv(self.client)\n",
    "        if self.convert_str: data = data.decode('utf8')\n",
    "        if self.use_pickle: data = pickle.loads(data)\n",
    "        if DEBUG_NETWORK: print('get_reply :', self.client, ' data :', data)\n",
    "        return data\n",
    "\n",
    "    def __del__(self):\n",
    "        self.client.close()\n",
    "        os.remove(self.self_unix_path)\n",
    "        return\n",
    "\n",
    "remote_uuid = uuid.uuid1().hex   # use uuid to identify threads\n",
    "\n",
    "unix_path = 'RECYCLE/Sockets/unix/%s'%remote_uuid\n",
    "server = UnixTcpServerP2P(unix_path, obj='pickle')\n",
    "client = UnixTcpClientP2P(unix_path, obj='pickle')\n",
    "\n",
    "def server_fn():\n",
    "    # data = server.wait_next_dgram()\n",
    "    # server.reply_last_client(np.array([4,5,6]))\n",
    "    while 1:\n",
    "        data = server.wait_next_dgram()\n",
    "        server.reply_last_client(data)\n",
    "\n",
    "def client_fn():\n",
    "    # rep = client.send_and_wait_reply(np.array([1,2,3]))\n",
    "    while True:\n",
    "        buf = np.random.rand(100,1000)\n",
    "        rep = client.send_and_wait_reply(buf)\n",
    "        assert (buf==rep).all()\n",
    "        print('成功')\n",
    "\n",
    "\n",
    "thread_hi = threading.Thread(target=server_fn)\n",
    "thread_hello = threading.Thread(target=client_fn)\n",
    "# 启动线程\n",
    "thread_hi.start()\n",
    "thread_hello.start()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'sadadwdwwfewfe@ fregergeeg#A5@ad'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b'sadadwdwwfewfe@ fregergeeg#A5@' + b'ad'\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "client.send_targeted_dgram('ddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "# from abc import abstractmethod\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 degenerate2ae = False,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        self.training = True\n",
    "\n",
    "        # Part 1, encoder\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Part 2, decoder\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.degenerate2ae = degenerate2ae\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        h_       = self.LeakyReLU(self.FC_input(input))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_) \n",
    "\n",
    "        return mean, log_var\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        h     = self.LeakyReLU(self.FC_hidden(z))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = self.FC_output(h)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        if not self.degenerate2ae:\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "        else:\n",
    "            z = mu\n",
    "        return  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "\n",
    "    def loss_function(self,\n",
    "                      x, x_hat, mean, log_var, kld_loss_weight, recons_loss_weight,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = x_hat\n",
    "        input = x\n",
    "        mu = mean\n",
    "        log_var = log_var\n",
    "\n",
    "        # kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss =F.mse_loss(recons, input)\n",
    "\n",
    "        # 计算高斯分布和标准正态分布的KL散度\n",
    "        if not self.degenerate2ae:\n",
    "            kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "            loss = recons_loss*recons_loss_weight + kld_loss*kld_loss_weight\n",
    "        else:\n",
    "            kld_loss = torch.zeros_like(recons_loss)\n",
    "            loss = recons_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "\n",
    "    # Do I need this?\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    # Do I need this?\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from UTILS.tensor_ops import _2tensor, cfg\n",
    "cfg.device_ = 'cuda:0'\n",
    "cfg.use_float64_ = False\n",
    "cfg.init = True\n",
    "\n",
    "input_dim = 10\n",
    "my_data_sample_x = np.random.rand(1000,10)*10 - 1 # random from -1 to +1\n",
    "my_data_sample_y = my_data_sample_x.mean(-1)**2 + my_data_sample_x.mean(-1)*(-2) + 1\n",
    "\n",
    "# solve the weight inital problem\n",
    "vae_mod = VanillaVAE(input_dim=input_dim, latent_dim=16, hidden_dim=32, degenerate2ae=False)\n",
    "my_data_sample_x = _2tensor(my_data_sample_x)\n",
    "vae_mod = _2tensor(vae_mod)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(vae_mod.parameters(), lr=3e-3)\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae_mod.train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    overall_loss = 0\n",
    "    x = my_data_sample_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "    x_hat, x_origin, mean, log_var = vae_mod(x) # model(x)\n",
    "    error = ( torch.abs(x_hat)-torch.abs(x) )/( torch.abs(x)+1e-9 )\n",
    "    std_, mean_ = torch.std_mean(error, unbiased=False)\n",
    "    print('\\test error mean %.2f and std %.2f'%(mean_.item(), std_.item()) )\n",
    "\n",
    "\n",
    "    lossdict = vae_mod.loss_function(x=x, x_hat=x_hat, mean=mean, log_var=log_var, kld_loss_weight=1, recons_loss_weight=1)\n",
    "    loss = lossdict['loss']\n",
    "    overall_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 'Reconstruction_Loss':recons_loss.detach(), 'KLD'\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tReconstruction_Loss: \", lossdict['Reconstruction_Loss'].item(), \"\\tKLD: \", lossdict['KLD'].item())\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, Tensor\n",
    "# from abc import abstractmethod\n",
    "\n",
    "\n",
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 degenerate2ae = False,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "        self.training = True\n",
    "\n",
    "        # Part 1, encoder\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # Part 2, decoder\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.degenerate2ae = degenerate2ae\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        h_       = self.LeakyReLU(self.FC_input(input))\n",
    "        h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_) \n",
    "\n",
    "        return mean, log_var\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        h     = self.LeakyReLU(self.FC_hidden(z))\n",
    "        h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        x_hat = self.FC_output(h)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs):\n",
    "        mu, log_var = self.encode(input)\n",
    "        if not self.degenerate2ae:\n",
    "            z = self.reparameterize(mu, log_var)\n",
    "        else:\n",
    "            z = mu\n",
    "        return  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "\n",
    "    def loss_function(self,\n",
    "                      x, x_hat, mean, log_var, kld_loss_weight, recons_loss_weight,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = x_hat\n",
    "        input = x\n",
    "        mu = mean\n",
    "        log_var = log_var\n",
    "\n",
    "        # kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        # 计算高斯分布和标准正态分布的KL散度\n",
    "        if not self.degenerate2ae:\n",
    "            kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "            loss = recons_loss*recons_loss_weight + kld_loss*kld_loss_weight\n",
    "        else:\n",
    "            kld_loss = torch.zeros_like(recons_loss)\n",
    "            loss = recons_loss\n",
    "\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "\n",
    "    # Do I need this?\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    # Do I need this?\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "import numpy as np\n",
    "from UTILS.tensor_ops import _2tensor, cfg\n",
    "cfg.device_ = 'cuda:0'\n",
    "cfg.use_float64_ = False\n",
    "cfg.init = True\n",
    "\n",
    "input_dim = 10\n",
    "my_data_sample_x = (np.random.rand(1000,10)-0.5)*2 + 5 # 0.75~1.25\n",
    "my_data_sample_y = my_data_sample_x.mean(-1)**2 + my_data_sample_x.mean(-1)*(-2) + 1\n",
    "\n",
    "# solve the weight inital problem\n",
    "vae_mod = VanillaVAE(input_dim=input_dim, latent_dim=16, hidden_dim=32, degenerate2ae=False)\n",
    "my_data_sample_x = _2tensor(my_data_sample_x)\n",
    "vae_mod = _2tensor(vae_mod)\n",
    "from torch.optim import Adam\n",
    "optimizer = Adam(vae_mod.parameters(), lr=3e-3)\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae_mod.train()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    overall_loss = 0\n",
    "    x = my_data_sample_x\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #  [self.decode(z), input, mu, log_var]    #  x_hat, mean, log_var\n",
    "    x_hat, x_origin, mean, log_var = vae_mod(x) # model(x)\n",
    "    error = ( torch.abs(x_hat - x) )/( torch.abs(x)+1e-9 )\n",
    "    std_, mean_ = torch.std_mean(error, unbiased=False)\n",
    "    print('\\test error mean %.2f%% and std %.2f'%(mean_.item()*100, std_.item()) , end='\\t')\n",
    "\n",
    "\n",
    "    lossdict = vae_mod.loss_function(x=x, x_hat=x_hat, mean=mean, log_var=log_var, kld_loss_weight=1, recons_loss_weight=1)\n",
    "    loss = lossdict['loss']\n",
    "    overall_loss += loss.item()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # 'Reconstruction_Loss':recons_loss.detach(), 'KLD'\n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tReconstruction_Loss: \", lossdict['Reconstruction_Loss'].item(), \"\\tKLD: \", lossdict['KLD'].item())\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
